{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import cPickle\n",
    "import copy\n",
    "from munkres import Munkres, print_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"/home/guillaume/Documents/CMU/cours/MT/git/sp2015.11-731/hw2/data\"\n",
    "embeddings_location_path = \"/home/guillaume/Documents/CMU/project/datasets/vectors/\"\n",
    "train_gold_path = os.path.join(data_path, \"train.gold\")\n",
    "tok_sentences_path = os.path.join(data_path, \"train-test.hyp1-hyp2-ref.tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26208 lines read from train gold file.\n"
     ]
    }
   ],
   "source": [
    "train_gold = np.array([[int(x) for x in line.strip().split()] for line in open(train_gold_path, 'r')])\n",
    "print \"%i lines read from train gold file.\" % len(train_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50339 lines read from tokenized sentences file.\n"
     ]
    }
   ],
   "source": [
    "sentences_tuples = [[s.split() for s in line.strip().lower().split(\" &#124; &#124; &#124; \")] for line in open(tok_sentences_path, 'r')]\n",
    "for s in sentences_tuples:\n",
    "    assert len(s) == 3\n",
    "print \"%i lines read from tokenized sentences file.\" % len(sentences_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17344 words in vocabulary.\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for s in sentences_tuples:\n",
    "    for sub_sentence in s:\n",
    "        for token in sub_sentence:\n",
    "            vocabulary.add(token)\n",
    "print \"%i words in vocabulary.\" % len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    embeddings = {}\n",
    "    token_not_in_index = 0\n",
    "    split_size = None\n",
    "    for line in open(os.path.join(embeddings_location_path, filename), 'r'):\n",
    "        temp = line.rstrip().split(\" \")\n",
    "        if split_size == None:\n",
    "            split_size = len(temp)\n",
    "        else:\n",
    "            assert len(temp) == split_size\n",
    "        token = temp[0].lower()\n",
    "        if token not in vocabulary and token not in [\"uuunkkk\", \"*unknown*\"]:\n",
    "            token_not_in_index += 1\n",
    "            continue\n",
    "        embeddings[token] = np.asarray([float(x) for x in temp[1:]]).astype(np.float32)\n",
    "    print '%i / %i words embeddings of dimension %i loaded from \"%s\". %i were not in the vocabulary.' % \\\n",
    "        (len(embeddings), len(vocabulary), split_size - 1, filename, token_not_in_index)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(a, b):\n",
    "    assert a.shape == b.shape\n",
    "    return np.sum(a*b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def w_cosine(a, b):\n",
    "    return cosine(embeddings[a], embeddings[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "penalty_coeff = 5\n",
    "def tokens_to_matrix(sentence1, sentence2):\n",
    "    sentence1 = [w for w in sentence1 if w in embeddings]\n",
    "    sentence2 = [w for w in sentence2 if w in embeddings]\n",
    "    #sentence1 = [w if w in embeddings else \"*unknown*\" for w in sentence1]\n",
    "    #sentence2 = [w if w in embeddings else \"*unknown*\" for w in sentence2]\n",
    "    n = len(sentence1)\n",
    "    m = len(sentence2)\n",
    "    matrix = np.zeros((n, m))\n",
    "    for i in xrange(n):\n",
    "        for j in xrange(m):\n",
    "            matrix[i][j] = w_cosine(sentence1[i], sentence2[j])\n",
    "            matrix[i][j] *= np.exp(-penalty_coeff * abs(float(i)/n - float(j)/m))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paraphrase_matrix(i):\n",
    "    [is_paraphrase, sentence1, sentence2] = train_lines[i]\n",
    "    return tokens_to_matrix(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_matrix(l):\n",
    "    return np.asarray(l)\n",
    "\n",
    "def matrix_to_list(m):\n",
    "    return [list(x) for x in m]\n",
    "\n",
    "def pad_matrix(matrix):\n",
    "    [n, m] = matrix.shape\n",
    "    if n == m:\n",
    "        return matrix\n",
    "    new_matrix = np.zeros((max(n, m), max(n, m)))\n",
    "    if n > m:\n",
    "        new_matrix[:, :m] = matrix\n",
    "    else:\n",
    "        new_matrix[:n, :] = matrix\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "factor = 1000000\n",
    "def best_matching(matrix):\n",
    "    original = matrix.copy()\n",
    "    # normalizing\n",
    "    mini = np.min(matrix)\n",
    "    if mini < 0:\n",
    "        matrix = matrix - 2 * mini\n",
    "    else:\n",
    "        mini = 0\n",
    "    matrix = (matrix * factor).astype(np.int32)\n",
    "    # convert matrix into list of list\n",
    "    matrix = matrix_to_list(matrix)\n",
    "    # we want the max sum, not min\n",
    "    cost_matrix = []\n",
    "    for row in matrix:\n",
    "        cost_row = []\n",
    "        for col in row:\n",
    "            cost_row += [sys.maxsize - col]\n",
    "        cost_matrix += [cost_row]\n",
    "    # run algorithm\n",
    "    m = Munkres()\n",
    "    indexes = m.compute(cost_matrix)\n",
    "    #print_matrix(matrix, msg='Highest profit through this matrix:')\n",
    "    total = 0.0\n",
    "    for row, column in indexes:\n",
    "        value = original[row][column]\n",
    "        total += value\n",
    "        #print '(%d, %d) -> %f' % (row, column, value)\n",
    "    # return result\n",
    "    #print 'total profit=%d' % total\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity_score(sentence1, sentence2):\n",
    "    matrix = tokens_to_matrix(sentence1, sentence2)\n",
    "    return best_matching(matrix) * 2 / (len(sentence1) + len(sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(scores):\n",
    "    count_correct = 0\n",
    "    for gold_score, score in zip(train_gold, scores):\n",
    "        count_correct += gold_score == score\n",
    "    accuracy = 1. * count_correct / min(len(train_gold), len(scores))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_scores(how=None):\n",
    "    scores = []\n",
    "    if how == None:\n",
    "        how = len(sentences_tuples)\n",
    "    for i in xrange(how):\n",
    "        hyp1, hyp2, ref = sentences_tuples[i]\n",
    "        new_score = 0\n",
    "        score1 = similarity_score(hyp1, ref)\n",
    "        score2 = similarity_score(hyp2, ref)\n",
    "        if score1 > score2:\n",
    "            new_score = -1\n",
    "        elif score1 < score2:\n",
    "            new_score = 1\n",
    "        #if new_score == 0:\n",
    "        #    print i\n",
    "        if i % 50 == 0 and i != 0:\n",
    "            print \".\",\n",
    "        if i == 2000:\n",
    "            break\n",
    "        scores.append(new_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "num_cores = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_score_sentence_parallel(i):\n",
    "    hyp1, hyp2, ref = sentences_tuples[i]\n",
    "    try:\n",
    "        score1 = similarity_score(hyp1, ref)\n",
    "    except:\n",
    "        score1 = 0\n",
    "    try:\n",
    "        score2 = similarity_score(hyp2, ref)\n",
    "    except:\n",
    "        score2 = 0\n",
    "    return (score1, score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new_tests = {\"glove.6B.300d.txt\":[3], \"google_300.txt\":[2, 3], \"huang_2012_vectors.txt\":[3, 4], \"levy-2014.txt\":[4], \"data8_A_100038words_1000vectors.txt\":[4]}\n",
    "#new_tests = {\"glove.6B.300d.txt\":[3], \"huang_2012_vectors.txt\":[3, 4], \"data8_A_100038words_1000vectors.txt\":[4]}\n",
    "new_tests = {\"google_300.txt\":[0.75]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12591 / 17344 words embeddings of dimension 300 loaded from \"google_300.txt\". 65862 were not in the vocabulary.\n",
      "0.75 [ 0.          0.00011447]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vectors_file, liste in new_tests.items():\n",
    "    embeddings = load_embeddings(vectors_file)\n",
    "    for penalty_coeff in liste:\n",
    "        scores = Parallel(n_jobs=num_cores)(delayed(compute_score_sentence_parallel)(i) for i in xrange(len(sentences_tuples)))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.54620726])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cPickle.dump(scores, open(\"/home/guillaume/Documents/CMU/cours/MT/git/sp2015.11-731/hw2/scores.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "model = kenlm.LanguageModel('/home/guillaume/Documents/CMU/project/datasets/rcv1.tok.1M.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyp1_scores = []\n",
    "hyp2_scores = []\n",
    "for i in xrange(len(sentences_tuples)):\n",
    "    hyp1, hyp2, ref = sentences_tuples[i]\n",
    "    hyp1_scores.append(model.score(\" \".join(hyp1)))\n",
    "    hyp2_scores.append(model.score(\" \".join(hyp2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "language_model_scores = [(a, b) for a, b in zip(np.array(hyp1_scores), np.array(hyp2_scores))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_scores(matching_scores, lm_scores, alpha):\n",
    "    final_scores = []\n",
    "    for i in xrange(len(sentences_tuples)):\n",
    "        score1 = (1 - alpha) * (matching_scores[i][0]) + alpha * lm_scores[i][0]\n",
    "        score2 = (1 - alpha) * (matching_scores[i][1]) + alpha * lm_scores[i][1]\n",
    "        if score1 > score2:\n",
    "            new_score = -1\n",
    "        elif score2 > score1:\n",
    "            new_score = 1\n",
    "        else:\n",
    "            new_score = 0\n",
    "        final_scores.append(new_score)\n",
    "    return final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_scores(matching_scores, lm_scores, nb_attempts):\n",
    "    for alpha in np.arange(0, 0.00001, 0.00001 / nb_attempts):\n",
    "        final_scores = merge_scores(matching_scores, lm_scores, alpha)\n",
    "        print alpha, compute_accuracy(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [ 0.54677961]\n",
      "1e-07 [ 0.54716117]\n",
      "2e-07 [ 0.54716117]\n",
      "3e-07 [ 0.54716117]\n",
      "4e-07 [ 0.54716117]\n",
      "5e-07 [ 0.54716117]\n",
      "6e-07 [ 0.54716117]\n",
      "7e-07 [ 0.54716117]\n",
      "8e-07 [ 0.54716117]\n",
      "9e-07 [ 0.54716117]\n",
      "1e-06 [ 0.54716117]\n",
      "1.1e-06 [ 0.54716117]\n",
      "1.2e-06 [ 0.54716117]\n",
      "1.3e-06 [ 0.54716117]\n",
      "1.4e-06 [ 0.54716117]\n",
      "1.5e-06 [ 0.54716117]\n",
      "1.6e-06 [ 0.54719933]\n",
      "1.7e-06 [ 0.54719933]\n",
      "1.8e-06 [ 0.54719933]\n",
      "1.9e-06 [ 0.54719933]\n",
      "2e-06 [ 0.54719933]\n",
      "2.1e-06 [ 0.54719933]\n",
      "2.2e-06 [ 0.54719933]\n",
      "2.3e-06 [ 0.54719933]\n",
      "2.4e-06 [ 0.54719933]\n",
      "2.5e-06 [ 0.54719933]\n",
      "2.6e-06 [ 0.54723748]\n",
      "2.7e-06 [ 0.54723748]\n",
      "2.8e-06 [ 0.54723748]\n",
      "2.9e-06 [ 0.54723748]\n",
      "3e-06 [ 0.54719933]\n",
      "3.1e-06 [ 0.54719933]\n",
      "3.2e-06 [ 0.54719933]\n",
      "3.3e-06 [ 0.54719933]\n",
      "3.4e-06 [ 0.54719933]\n",
      "3.5e-06 [ 0.54719933]\n",
      "3.6e-06 [ 0.54719933]\n",
      "3.7e-06 [ 0.54719933]\n",
      "3.8e-06 [ 0.54719933]\n",
      "3.9e-06 [ 0.54719933]\n",
      "4e-06 [ 0.54719933]\n",
      "4.1e-06 [ 0.54719933]\n",
      "4.2e-06 [ 0.54719933]\n",
      "4.3e-06 [ 0.54719933]\n",
      "4.4e-06 [ 0.54719933]\n",
      "4.5e-06 [ 0.54719933]\n",
      "4.6e-06 [ 0.54719933]\n",
      "4.7e-06 [ 0.54716117]\n",
      "4.8e-06 [ 0.54712302]\n",
      "4.9e-06 [ 0.54712302]\n",
      "5e-06 [ 0.54712302]\n",
      "5.1e-06 [ 0.54712302]\n",
      "5.2e-06 [ 0.54712302]\n",
      "5.3e-06 [ 0.54712302]\n",
      "5.4e-06 [ 0.54712302]\n",
      "5.5e-06 [ 0.54712302]\n",
      "5.6e-06 [ 0.54712302]\n",
      "5.7e-06 [ 0.54712302]\n",
      "5.8e-06 [ 0.54712302]\n",
      "5.9e-06 [ 0.54712302]\n",
      "6e-06 [ 0.54712302]\n",
      "6.1e-06 [ 0.54712302]\n",
      "6.2e-06 [ 0.54712302]\n",
      "6.3e-06 [ 0.54712302]\n",
      "6.4e-06 [ 0.54712302]\n",
      "6.5e-06 [ 0.54712302]\n",
      "6.6e-06 [ 0.54712302]\n",
      "6.7e-06 [ 0.54712302]\n",
      "6.8e-06 [ 0.54712302]\n",
      "6.9e-06 [ 0.54712302]\n",
      "7e-06 [ 0.54712302]\n",
      "7.1e-06 [ 0.54712302]\n",
      "7.2e-06 [ 0.54712302]\n",
      "7.3e-06 [ 0.54712302]\n",
      "7.4e-06 [ 0.54712302]\n",
      "7.5e-06 [ 0.54712302]\n",
      "7.6e-06 [ 0.54712302]\n",
      "7.7e-06 [ 0.54712302]\n",
      "7.8e-06 [ 0.54712302]\n",
      "7.9e-06 [ 0.54712302]\n",
      "8e-06 [ 0.54712302]\n",
      "8.1e-06 [ 0.54712302]\n",
      "8.2e-06 [ 0.54712302]\n",
      "8.3e-06 [ 0.54712302]\n",
      "8.4e-06 [ 0.54712302]\n",
      "8.5e-06 [ 0.54712302]\n",
      "8.6e-06 [ 0.54712302]\n",
      "8.7e-06 [ 0.54712302]\n",
      "8.8e-06 [ 0.54712302]\n",
      "8.9e-06 [ 0.54712302]\n",
      "9e-06 [ 0.54712302]\n",
      "9.1e-06 [ 0.54712302]\n",
      "9.2e-06 [ 0.54712302]\n",
      "9.3e-06 [ 0.54712302]\n",
      "9.4e-06 [ 0.54712302]\n",
      "9.5e-06 [ 0.54712302]\n",
      "9.6e-06 [ 0.54712302]\n",
      "9.7e-06 [ 0.54712302]\n",
      "9.8e-06 [ 0.54712302]\n",
      "9.9e-06 [ 0.54712302]\n"
     ]
    }
   ],
   "source": [
    "combine_scores(scores, language_model_scores, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_scores = merge_scores(scores, language_model_scores, 2.8e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/guillaume/Documents/CMU/cours/MT/git/sp2015.11-731/hw2/output.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join([str(x) for x in final_scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
