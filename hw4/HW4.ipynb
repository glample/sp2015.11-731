{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/guillaume/workspace/NewDeep\n"
     ]
    }
   ],
   "source": [
    "%cd '/home/guillaume/workspace/NewDeep/'\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import os\n",
    "import sys\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import utils\n",
    "import nbests\n",
    "import codecs\n",
    "import collections\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/home/guillaume/Documents/CMU/cours/MT/git/sp2015.11-731/hw4/data\"\n",
    "bilingual_vectors_cs_path = \"/home/guillaume/workspace/NewDeep/word_vectors/OutFileEE_orig1_projected.txt\"\n",
    "bilingual_vectors_en_path = \"/home/guillaume/workspace/NewDeep/word_vectors/OutFileEE_orig2_projected.txt\"\n",
    "\n",
    "bilingual_vectors_cs_path = \"/home/guillaume/workspace/NewDeep/word_vectors/cca.cs.50d.txt\"\n",
    "bilingual_vectors_en_path = \"/home/guillaume/workspace/NewDeep/word_vectors/cca.en.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input_path = os.path.join(data_path, \"train.input\")\n",
    "train_refs_path = os.path.join(data_path, \"train.refs\")\n",
    "\n",
    "dev_test_input_path = os.path.join(data_path, \"dev+test.input\")\n",
    "dev_refs_path = os.path.join(data_path, \"dev.refs\")\n",
    "\n",
    "ttable_path = os.path.join(data_path, \"ttable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_vectors(filename):\n",
    "    lines = [line.rstrip().split() for line in codecs.open(filename, 'r', \"utf8\")]\n",
    "    if len(lines[0]) == 2:\n",
    "        lines.pop(0)\n",
    "    nb_words = len(lines)\n",
    "    embedding_dimension = None\n",
    "    words = []\n",
    "    for i, line in enumerate(lines):\n",
    "        if embedding_dimension == None:\n",
    "            embedding_dimension = len(line) - 1\n",
    "            vectors = np.zeros((nb_words, embedding_dimension))\n",
    "        else:\n",
    "            if not len(line) - 1 == embedding_dimension:\n",
    "                print \"Problem line %i.. skipped\" % i\n",
    "                continue\n",
    "        words.append(line[0])\n",
    "        vectors[i] = np.array([float(x) for x in line[1:]])\n",
    "    return words, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202089, 50)\n"
     ]
    }
   ],
   "source": [
    "cs_words, cs_vectors = read_vectors(bilingual_vectors_cs_path)\n",
    "print cs_vectors.shape\n",
    "external_cs_voc = set(cs_words)\n",
    "mapping_to_ext_cs = {}\n",
    "for i, w in enumerate(cs_words):\n",
    "    mapping_to_ext_cs[w] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86330, 50)\n"
     ]
    }
   ],
   "source": [
    "en_words, en_vectors = read_vectors(bilingual_vectors_en_path)\n",
    "print en_vectors.shape\n",
    "external_en_voc = set(en_words)\n",
    "mapping_to_ext_en = {}\n",
    "for i, w in enumerate(en_words):\n",
    "    mapping_to_ext_en[w] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122262 / 5791 / 5791 lines read from train / dev / ref inputs\n",
      "122262 / 5791 lines read from train / dev refs\n",
      "334770 lines read from ttable\n"
     ]
    }
   ],
   "source": [
    "train_input_lines = [[x.strip().split(\" \") for x in line.rstrip().split(\"|||\")] for line in codecs.open(train_input_path, 'r', \"utf8\")]\n",
    "dev_test_input_lines = [[x.strip().split(\" \") for x in line.rstrip().split(\"|||\")] for line in codecs.open(dev_test_input_path, 'r', \"utf8\")]\n",
    "for line in train_input_lines + dev_test_input_lines:\n",
    "    assert len(line) == 3\n",
    "\n",
    "dev_input_lines = dev_test_input_lines[:5791]\n",
    "test_input_lines = dev_test_input_lines[5791:]\n",
    "\n",
    "train_refs_lines = [line.rstrip() for line in open(train_refs_path, 'r')]\n",
    "dev_refs_lines = [line.rstrip() for line in open(dev_refs_path, 'r')]\n",
    "\n",
    "ttable_lines = [[x.strip().split(\" \") for x in line.rstrip().split(\"|||\")] for line in open(ttable_path, 'r')]\n",
    "\n",
    "print \"%i / %i / %i lines read from train / dev / ref inputs\" % (len(train_input_lines), len(dev_input_lines), len(test_input_lines))\n",
    "print \"%i / %i lines read from train / dev refs\" % (len(train_refs_lines), len(dev_refs_lines))\n",
    "print \"%i lines read from ttable\" % len(ttable_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_voc(lines):\n",
    "    voc = {}\n",
    "    for line in lines:\n",
    "        for token in line[0] + line[1] + line[2]:\n",
    "            if token in voc:\n",
    "                voc[token] += 1\n",
    "            else:\n",
    "                voc[token] = 1\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46566 / 14648 / 14596 / 21460 different tokens in train / dev / test / ttable\n"
     ]
    }
   ],
   "source": [
    "train_english_tok_count = get_voc(train_input_lines)\n",
    "dev_english_tok_count = get_voc(dev_input_lines)\n",
    "test_english_tok_count = get_voc(test_input_lines)\n",
    "\n",
    "train_english_tok_count = {k:v for k, v in train_english_tok_count.items() if k in external_en_voc}\n",
    "\n",
    "train_english_tok_count[\"__EMPTY__\"] = 0\n",
    "\n",
    "train_english_voc = set(train_english_tok_count.keys())\n",
    "dev_english_voc = set(dev_english_tok_count.keys())\n",
    "test_english_voc = set(test_english_tok_count.keys())\n",
    "\n",
    "ttable_tok_count = {}\n",
    "for line in ttable_lines:\n",
    "    for token in line[0]:\n",
    "        if token in ttable_tok_count:\n",
    "            ttable_tok_count[token] += 1\n",
    "        else:\n",
    "            ttable_tok_count[token] = 1\n",
    "ttable_voc = set(ttable_tok_count.keys())\n",
    "\n",
    "print \"%i / %i / %i / %i different tokens in train / dev / test / ttable\" % (len(train_english_voc), len(dev_english_voc), len(test_english_voc), len(ttable_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for w, c in train_english_tok_count.items():\\n    if c < 20000 and not w.isdigit():\\n        print w'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for w, c in train_english_tok_count.items():\n",
    "    if c < 20000 and not w.isdigit():\n",
    "        print w\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_index_en, index_to_word_en = utils.get_indexes(train_english_tok_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334770\n",
      "{1: 198512, 2: 122291, 3: 12374, 4: 1344, 5: 208, 6: 26, 7: 5, 8: 4, 9: 1, 10: 4, 11: 1}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "sizes = {}\n",
    "for line in ttable_lines:\n",
    "    line = line[1]\n",
    "    if len(line) not in sizes:\n",
    "        sizes[len(line)] = 1\n",
    "    else:\n",
    "        sizes[len(line)] += 1\n",
    "    count += 1\n",
    "print count\n",
    "print sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55237 different phrases found in the phrase table for Czech, with 486915 in total.\n",
      "97371 elements read from the phrase table, with 331860 different pairs of phrases.\n",
      "2910 phrases were ignored.\n"
     ]
    }
   ],
   "source": [
    "cs_voc_count = {}\n",
    "for line in ttable_lines:\n",
    "    assert len(line) == 3\n",
    "    for token in line[1]:\n",
    "        token = token.decode(\"utf8\")\n",
    "        if token in cs_voc_count:\n",
    "            cs_voc_count[token] += 1\n",
    "        else:\n",
    "            cs_voc_count[token] = 1\n",
    "word_to_index_cs, index_to_word_cs = utils.get_indexes(cs_voc_count)\n",
    "\n",
    "ttable = {}\n",
    "count_ignored = 0\n",
    "for line in ttable_lines:\n",
    "    assert len(line) == 3\n",
    "    # if the word phrase is not in our dictionary\n",
    "    should_break = False\n",
    "    for token in line[0]:\n",
    "        if token not in word_to_index_en:\n",
    "            should_break = True\n",
    "    if should_break:\n",
    "        count_ignored += 1\n",
    "        continue\n",
    "    phrase_en = tuple([token.decode(\"utf8\") for token in line[0]])\n",
    "    phrase_cs = [token.decode(\"utf8\") for token in line[1]]\n",
    "    scores = np.array([float(x) for x in line[2]], dtype=np.float32)\n",
    "    if phrase_en in ttable:\n",
    "        ttable[phrase_en].append((phrase_cs, scores))\n",
    "    else:\n",
    "        ttable[phrase_en] = [(phrase_cs, scores)]\n",
    "\n",
    "print \"%i different phrases found in the phrase table for Czech, with %i in total.\" % (len(word_to_index_cs), sum(cs_voc_count.values()))\n",
    "print \"%i elements read from the phrase table, with %i different pairs of phrases.\" % (len(ttable), sum([len(x) for x in ttable.values()]))\n",
    "print \"%i phrases were ignored.\" % count_ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absent in train: 0\n",
      "\n",
      "Absent in dev: 0\n",
      "\n",
      "Absent in test: 0\n",
      "\n",
      "\n",
      "Absent in train: 0\n",
      "\n",
      "Absent in dev: 0\n",
      "\n",
      "Absent in test: 0\n"
     ]
    }
   ],
   "source": [
    "# every phrase we are interested in contains words in the dictionary\n",
    "count_absent = 0\n",
    "for i, (_, phrase, _) in enumerate(train_input_lines):\n",
    "    for token in phrase:\n",
    "        if not token in word_to_index_en:\n",
    "            count_absent += 1\n",
    "            print i, token, phrase\n",
    "print \"Absent in train: %i\\n\" % count_absent\n",
    "\n",
    "count_absent = 0\n",
    "for i, (_, phrase, _) in enumerate(dev_input_lines):\n",
    "    for token in phrase:\n",
    "        if not token in word_to_index_en:\n",
    "            count_absent += 1\n",
    "            print i, token, phrase\n",
    "print \"Absent in dev: %i\\n\" % count_absent\n",
    "\n",
    "count_absent = 0\n",
    "for i, (_, phrase, _) in enumerate(test_input_lines):\n",
    "    for token in phrase:\n",
    "        if not token in word_to_index_en:\n",
    "            count_absent += 1\n",
    "            print i, token, phrase\n",
    "print \"Absent in test: %i\\n\\n\" % count_absent\n",
    "\n",
    "\n",
    "# every phrase we are interested in is in the ttable\n",
    "count_absent = 0\n",
    "for i, (_, phrase, _) in enumerate(train_input_lines):\n",
    "    if not tuple(phrase) in ttable:\n",
    "        count_absent += 1\n",
    "        print i, phrase\n",
    "print \"Absent in train: %i\\n\" % count_absent\n",
    "\n",
    "count_absent = 0\n",
    "for i, (_, phrase, _) in enumerate(dev_input_lines):\n",
    "    if not tuple(phrase) in ttable:\n",
    "        count_absent += 1\n",
    "        print i, phrase\n",
    "print \"Absent in dev: %i\\n\" % count_absent\n",
    "\n",
    "count_absent = 0\n",
    "for i, (_, phrase, _) in enumerate(test_input_lines):\n",
    "    if not tuple(phrase) in ttable:\n",
    "        count_absent += 1\n",
    "        print i, phrase\n",
    "print \"Absent in test: %i\" % count_absent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input_lines = [l for i, l in enumerate(train_input_lines) if i != 34612]\n",
    "dev_input_lines = [l for i, l in enumerate(dev_input_lines) if i != 356 and i != 5421]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 55237\n"
     ]
    }
   ],
   "source": [
    "count_absent = 0\n",
    "for word in word_to_index_cs:\n",
    "    if not word in external_cs_voc:\n",
    "        count_absent += 1\n",
    "print count_absent, len(word_to_index_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1281\n",
      "5752152\n",
      "{1: 4491698, 2: 541375, 3: 54611, 4: 3593, 5: 156}\n"
     ]
    }
   ],
   "source": [
    "count_absent = 0\n",
    "count_present = 0\n",
    "sizes = {}\n",
    "for _, phrase, _ in train_input_lines + dev_input_lines + test_input_lines:\n",
    "    candidates = ttable[tuple(phrase)]\n",
    "    for tokens in [candidate[0] for candidate in candidates]:\n",
    "        if len(tokens) in sizes:\n",
    "            sizes[len(tokens)] += 1\n",
    "        else:\n",
    "            sizes[len(tokens)] = 1\n",
    "        for token in tokens:\n",
    "            if not token in external_cs_voc:\n",
    "                count_absent += 1\n",
    "            else:\n",
    "                count_present += 1\n",
    "print count_absent\n",
    "print count_present\n",
    "print sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 50000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122261 / 5789 / 5791 elements in the train / dev / test.\n"
     ]
    }
   ],
   "source": [
    "x_train = [[[x for x in l if x in word_to_index_en and (train_english_tok_count[x] < threshold or i == 1)] for i, l in enumerate(sentence)] for sentence in train_input_lines]\n",
    "x_dev = [[[x for x in l if x in word_to_index_en and (train_english_tok_count[x] < threshold or i == 1)] for i, l in enumerate(sentence)] for sentence in dev_input_lines]\n",
    "x_test = [[[x for x in l if x in word_to_index_en and (train_english_tok_count[x] < threshold or i == 1)] for i, l in enumerate(sentence)] for sentence in test_input_lines]\n",
    "\n",
    "\"\"\"x_train = [(sentence[0] + sentence[2], sentence[1]) for sentence in x_train]\n",
    "x_dev = [(sentence[0] + sentence[2], sentence[1]) for sentence in x_dev]\n",
    "x_test = [(sentence[0] + sentence[2], sentence[1]) for sentence in x_test]\n",
    "\n",
    "x_train = [[l if len(l) > 0 else [word_to_index_en[\"__EMPTY__\"]] for l in sentence] for sentence in x_train]\n",
    "x_dev = [[l if len(l) > 0 else [word_to_index_en[\"__EMPTY__\"]] for l in sentence] for sentence in x_dev]\n",
    "x_test = [[l if len(l) > 0 else [word_to_index_en[\"__EMPTY__\"]] for l in sentence] for sentence in x_test]\"\"\"\n",
    "\n",
    "print \"%i / %i / %i elements in the train / dev / test.\" % (len(x_train), len(x_dev), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5821 0\n"
     ]
    }
   ],
   "source": [
    "countok = 0\n",
    "countnok = 0\n",
    "for i, (a, b, c) in enumerate(x_test):\n",
    "    for w in b:\n",
    "        if not w in external_en_voc:\n",
    "            countnok += 1\n",
    "        else:\n",
    "            countok += 1\n",
    "print countok, countnok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" there have even been\n",
      "calls\n",
      "to julian assange . \"\n",
      "\n",
      "said that assange was therefore\n",
      "afraid\n",
      "for his personal safety .\n",
      "\n",
      "the wikileaks\n",
      "founder\n",
      "will remain in hiding .\n",
      "\n",
      "was clearly reacting to statements from north\n",
      "america\n",
      ".\n",
      "\n",
      "in america , the former republican governor of arkansas , mike , had called for the people\n",
      "responsible\n",
      "for the wikileaks revelations to be indicted and executed for treason .\n",
      "\n",
      "in the meantime , exchanges have been made between the white\n",
      "house\n",
      "and assange .\n",
      "\n",
      "robert , the speaker of american president barack obama , said that it was \" ridiculous and absurd \" that assange had demanded the resignation of secretary of state hillary clinton if she is found\n",
      "responsible\n",
      "for recently revealed requests to diplomats to engage in espionage .\n",
      "\n",
      "\" i 'm not entirely sure why we care about the\n",
      "opinion\n",
      "of a guy with a website , \" said .\n",
      "\n",
      "\" our foreign policy and the interests of this country are far stronger than his one\n",
      "website\n",
      ". \"\n",
      "\n",
      "the american\n",
      "government\n",
      "is currently carrying out a comprehensive review of the security of its databases .\n",
      "\n",
      "russell travers , deputy director of the information division at the national anti - terrorism centre , has been appointed as a special representative , the white\n",
      "house\n",
      "announced .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (a, b, c) in enumerate(x_test):\n",
    "    print \" \".join(a)\n",
    "    print \" \".join(b)\n",
    "    print \" \".join(c)\n",
    "    print\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "pb = 0\n",
    "for a, b, c in x_test:\n",
    "    if a == [word_to_index_en[\"__EMPTY__\"]]:\n",
    "        count += 1\n",
    "    if c == [word_to_index_en[\"__EMPTY__\"]]:\n",
    "        count += 1\n",
    "    assert b != [word_to_index_en[\"__EMPTY__\"]]\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122261"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122261 / 5789 references in the train / dev.\n"
     ]
    }
   ],
   "source": [
    "y_train = [phrase.decode(\"utf-8\").split() for phrase in train_refs_lines]\n",
    "y_train = [l for i, l in enumerate(y_train) if i != 34612]\n",
    "y_dev = [phrase.decode(\"utf-8\").split() for phrase in dev_refs_lines]\n",
    "y_dev = [l for i, l in enumerate(y_dev) if i != 356 and i != 5421]\n",
    "\n",
    "print \"%i / %i references in the train / dev.\" % (len(y_train), len(y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# Training parameters #\n",
    "#######################\n",
    "voc_size_en = len(index_to_word_en)\n",
    "voc_size_cs = len(index_to_word_cs)\n",
    "\n",
    "en_dim = 50\n",
    "cs_dim = 50\n",
    "\n",
    "k_top = 4\n",
    "\n",
    "nb_filters = 4\n",
    "filter_height = 6\n",
    "filter_width = 2\n",
    "\n",
    "layers_learning_rate = 0.01\n",
    "embedding_learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import layers\n",
    "import networks\n",
    "import conv_sentences\n",
    "\n",
    "reload(layers)\n",
    "reload(networks)\n",
    "reload(conv_sentences)\n",
    "\n",
    "############\n",
    "# Training #\n",
    "############\n",
    "\n",
    "# inputs\n",
    "context_input = T.imatrix(name='context_input')\n",
    "phrase_input = T.imatrix(name='phrase_input')\n",
    "\n",
    "# outputs\n",
    "good_phrase = T.imatrix(name='phrase_good_output')\n",
    "good_probs = T.fmatrix(name='phrase_good_probs')\n",
    "bad_phrase = T.imatrix(name='phrase_bad_output')\n",
    "bad_probs = T.fmatrix(name='phrase_bad_probs')\n",
    "\n",
    "# is good\n",
    "is_good = T.fvector(name=\"is_good\")\n",
    "\n",
    "# embeddings\n",
    "embeddings_layer_en = layers.Embedding_layer(name='embedding_en', voc_size=voc_size_en, embedding_dim=en_dim, learning_rate=embedding_learning_rate)\n",
    "embeddings_layer_cs = layers.Embedding_layer(name='embedding_cs', voc_size=voc_size_cs, embedding_dim=cs_dim, learning_rate=embedding_learning_rate)\n",
    "\n",
    "# hidden layer\n",
    "hidden_layer = layers.Biased_hidden_layer(name='hidden_layer', learning_rate=layers_learning_rate, input_dim=en_dim + 100 + 4, output_dim=10, activation=T.nnet.sigmoid)\n",
    "#hidden_layer.weights.set_value(np.array(hidden_layer.weights.get_value(), dtype=np.float32))\n",
    "\n",
    "# hidden layer 2\n",
    "# dden_layer2 = layers.Biased_hidden_layer(name='hidden_layer_2', learning_rate=layers_learning_rate, input_dim=cs_dim, output_dim=1, activation=T.nnet.sigmoid)\n",
    "\n",
    "# merge scores\n",
    "merge_layer = layers.Biased_hidden_layer(name='merge_scores_layer', learning_rate=layers_learning_rate, input_dim=10, output_dim=1, activation=T.nnet.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors_values_en = embeddings_layer_en.weights.get_value()\n",
    "for word in word_to_index_en:\n",
    "    if word in mapping_to_ext_en:\n",
    "        vectors_values_en[word_to_index_en[word]] = en_vectors[mapping_to_ext_en[word]]\n",
    "embeddings_layer_en.weights.set_value(vectors_values_en)\n",
    "\n",
    "vectors_values_cs = embeddings_layer_cs.weights.get_value()\n",
    "for word in word_to_index_cs:\n",
    "    if word in mapping_to_ext_cs:\n",
    "        vectors_values_cs[word_to_index_cs[word]] = cs_vectors[mapping_to_ext_cs[word]]\n",
    "embeddings_layer_cs.weights.set_value(vectors_values_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  3 50]\n",
      "[11  2 50]\n",
      "[11 50]\n",
      "[11 50]\n",
      "[11 50]\n",
      "[ 11 100]\n",
      "[11  4]\n",
      "[ 11 154]\n",
      "[11 10]\n",
      "[11]\n",
      "0.733224511147\n"
     ]
    }
   ],
   "source": [
    "# link\n",
    "test_context = np.array(np.random.randint(0, 10, (11, 3)), dtype=np.int32)\n",
    "test_phrase = np.array(np.random.randint(0, 10, (11, 2)), dtype=np.int32)\n",
    "\n",
    "test_good_phrase = np.array(np.random.randint(0, 10, (11, 1)), dtype=np.int32)\n",
    "test_bad_phrase = np.array(np.random.randint(0, 10, (11, 1)), dtype=np.int32)\n",
    "\n",
    "test_good_probas = np.array(np.random.rand(11, 4), dtype=np.float32)\n",
    "test_bad_probas = np.array(np.random.rand(11, 4), dtype=np.float32)\n",
    "\n",
    "test_is_good = np.array(np.random.rand(11,), dtype=np.float32)\n",
    "\n",
    "# we create a dimension that corresponds to a single feature map (en)\n",
    "inputs_shape_en = T.join(0, [context_input.shape[0]], [context_input.shape[1]], [en_dim])\n",
    "inputs_context = embeddings_layer_en.link([context_input])[0].reshape(inputs_shape_en, ndim=3)#.dimshuffle(0, 'x', 1, 2)\n",
    "\n",
    "inputs_shape_phrase_en = T.join(0, [phrase_input.shape[0]], [phrase_input.shape[1]], [en_dim])\n",
    "inputs_phrase = embeddings_layer_en.link([phrase_input])[0].reshape(inputs_shape_phrase_en, ndim=3)\n",
    "\n",
    "\n",
    "print inputs_context.shape.eval({context_input:test_context})\n",
    "print inputs_phrase.shape.eval({phrase_input:test_phrase})\n",
    "\n",
    "inputs_context = T.mean(inputs_context, axis=1)\n",
    "inputs_phrase = T.mean(inputs_phrase, axis=1)\n",
    "\n",
    "print inputs_context.shape.eval({context_input:test_context})\n",
    "print inputs_phrase.shape.eval({phrase_input:test_phrase})\n",
    "\n",
    "\n",
    "# we create a dimension that corresponds to a single feature map (cs)\n",
    "inputs_shape_cs = T.join(0, [good_phrase.shape[0]], [good_phrase.shape[1]], [cs_dim])\n",
    "good_output = embeddings_layer_cs.link([good_phrase])[0].reshape(inputs_shape_cs, ndim=3)\n",
    "good_output = T.mean(good_output, axis=1)\n",
    "print good_output.shape.eval({good_phrase:test_good_phrase})\n",
    "\n",
    "\n",
    "\n",
    "# compute the scores\n",
    "norms = T.sqrt((inputs_context ** 2).sum(axis=1) * (good_output ** 2).sum(axis=1))\n",
    "context_score = ((inputs_context * good_output).sum(axis=1) / norms).dimshuffle(0, 'x').repeat(100, axis=1)\n",
    "print context_score.shape.eval({context_input:test_context, good_phrase:test_good_phrase})\n",
    "#print context_score.repeat(10, axis=1).shape.eval({context_input:test_context, good_phrase:test_good_phrase})\n",
    "\n",
    "\n",
    "#print good_score.shape.eval({context_input:test_context, good_phrase:test_good_phrase})\n",
    "print good_probs.shape.eval({good_probs:test_good_probas})\n",
    "\n",
    "#merged_good_scores = merge_layer.link([T.concatenate([good_output.dimshuffle(0, 'x'), good_probs], axis=1)])[0].flatten()\n",
    "merged_good_scores = T.concatenate([context_score, inputs_phrase, good_probs], axis=1)\n",
    "\n",
    "print merged_good_scores.shape.eval({context_input:test_context, phrase_input:test_phrase, good_phrase:test_good_phrase, good_probs:test_good_probas})\n",
    "\n",
    "\n",
    "hidden_scores = hidden_layer.link([merged_good_scores])\n",
    "print hidden_scores[0].shape.eval({context_input:test_context, phrase_input:test_phrase, good_phrase:test_good_phrase, good_probs:test_good_probas})\n",
    "\n",
    "\n",
    "final_score = merge_layer.link(hidden_scores)[0].flatten()\n",
    "print final_score.shape.eval({context_input:test_context, phrase_input:test_phrase, good_phrase:test_good_phrase, good_probs:test_good_probas})\n",
    "\n",
    "\n",
    "\n",
    "cost = - T.mean(T.log( (1 - is_good) + (2 * is_good - 1) * final_score ))\n",
    "\n",
    "\n",
    "print cost.eval({is_good:test_is_good, context_input:test_context, phrase_input:test_phrase, good_phrase:test_good_phrase, good_probs:test_good_probas})\n",
    "\n",
    "#merged_bad_scores = theano.clone(merged_good_scores, {good_phrase:bad_phrase, good_probs:bad_probs})\n",
    "#merged_bad_scores = merge_layer.link([bad_probs])[0].flatten()\n",
    "#print merged_bad_scores.eval({left_input:test_l, right_input:test_r, phrase_input:test_p, bad_phrase:test_b, bad_probs:test_bp})\n",
    "\n",
    "\n",
    "#final_score = (0.5 - merged_good_scores + merged_bad_scores)\n",
    "#cost = (final_score * (final_score > 0)).mean()\n",
    "\n",
    "#print final_score.eval({left_input:test_l, right_input:test_r, phrase_input:test_p, good_phrase:test_g, good_probs:test_gp, bad_phrase:test_b, bad_probs:test_bp})\n",
    "#print final_score.eval({left_input:test_l, right_input:test_r, good_phrase:test_g, good_probs:test_gp, bad_phrase:test_b, bad_probs:test_bp})\n",
    "#print cost.eval({left_input:test_l, right_input:test_r, good_phrase:test_g, good_probs:test_gp, bad_phrase:test_b, bad_probs:test_bp})\n",
    "#print final_score.eval({left_input:test_l, right_input:test_r, good_phrase:test_g, good_probs:test_gp, bad_phrase:test_b, bad_probs:test_bp})\n",
    "#print cost.eval({left_input:test_l, right_input:test_r, good_phrase:test_g, good_probs:test_gp, bad_phrase:test_b, bad_probs:test_bp})\n",
    "#print final_score.eval({good_probs:test_gp,  bad_probs:test_bp})\n",
    "\n",
    "#print merged_good_scores.eval({left_input:test_l, right_input:test_r, good_phrase:test_g, good_probs:test_gp})\n",
    "#print cost.eval({left_input:test_l, right_input:test_r, good_phrase:test_g, good_probs:test_gp, is_good:test_ig})\n",
    "#print cost.eval({good_probs:test_gp, is_good:test_ig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updates(params, cost):\n",
    "    gradients = T.grad(cost=cost, wrt=params)\n",
    "    to_return = []\n",
    "    for p, g in zip(params, gradients):\n",
    "        learning_rate = embedding_learning_rate if str(p) == \"embeddings\" else layers_learning_rate\n",
    "        to_return.append((p, p - learning_rate * g))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfunc_feedforward = theano.function(\\n    inputs=[good_probs],\\n    outputs=merged_good_scores\\n)\\n\\nfunc_train = theano.function(\\n    inputs=[good_probs, is_good],\\n    outputs=cost,\\n    updates=updates(params, cost)\\n)\\n'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build\n",
    "\n",
    "params = merge_layer.params + hidden_layer.params + embeddings_layer_cs.params + embeddings_layer_en.params# + hidden_layer.params + conv_layer.params\n",
    "\n",
    "\n",
    "func_feedforward = theano.function(\n",
    "    inputs=[context_input, phrase_input, good_phrase, good_probs],\n",
    "    outputs=final_score\n",
    ")\n",
    "\n",
    "func_train = theano.function(\n",
    "    #inputs=[left_input, right_input, good_phrase, good_probs, bad_phrase, bad_probs],\n",
    "    inputs=[context_input, phrase_input, good_phrase, good_probs, is_good],\n",
    "    outputs=cost,\n",
    "    updates=updates(params, cost)\n",
    ")\n",
    "\"\"\"\n",
    "func_feedforward = theano.function(\n",
    "    inputs=[good_probs],\n",
    "    outputs=merged_good_scores\n",
    ")\n",
    "\n",
    "func_train = theano.function(\n",
    "    inputs=[good_probs, is_good],\n",
    "    outputs=cost,\n",
    "    updates=updates(params, cost)\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.89995199,  0.89721417,  0.96563375,  0.98623151,  0.93535578,\n",
       "        0.81931043,  0.89114976,  0.91361487,  0.88886243,  0.86775804,\n",
       "        0.88542515], dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_feedforward(test_context, test_phrase, test_bad_phrase, test_bad_probas)\n",
    "#func_feedforward(test_b, test_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in xrange(10):\\n    if i % 1 == 0:\\n        print func_train(test_context, test_good_phrase, test_good_probas, test_is_good)\\n    else:\\n        pass\\n        #func_train(test_context, test_good_probas, test_bad, test_b)'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for i in xrange(10):\n",
    "    if i % 1 == 0:\n",
    "        print func_train(test_context, test_good_phrase, test_good_probas, test_is_good)\n",
    "    else:\n",
    "        pass\n",
    "        #func_train(test_context, test_good_probas, test_bad, test_b)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers_learning_rate = 0.03\n",
    "embedding_learning_rate = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"/home/guillaume/Documents/CMU/cours/MT/git/sp2015.11-731/hw4/models/threshold20k_ctxt3_train_emb_epoch_%i_%s_%s_%f_%f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_score_train, model_score_test = [0.527439, 0.397632]\n",
    "model_epoch = 172789\n",
    "\n",
    "hidden_layer.weights.set_value(cPickle.load(open(model_path % (model_epoch, \"sh_first\", \"hidden_weights\", model_score_train, model_score_test), \"r\")))\n",
    "hidden_layer.bias.set_value(cPickle.load(open(model_path % (model_epoch, \"sh_first\", \"hidden_bias\", model_score_train, model_score_test), \"r\")))\n",
    "\n",
    "merge_layer.weights.set_value(cPickle.load(open(model_path % (model_epoch, \"sh_first\", \"merge_weights\", model_score_train, model_score_test), \"r\")))\n",
    "merge_layer.bias.set_value(cPickle.load(open(model_path % (model_epoch, \"sh_first\", \"merge_bias\", model_score_train, model_score_test), \"r\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad(words, left, final_size=3):\n",
    "    if len(words) == final_size:\n",
    "        return words\n",
    "    elif left:\n",
    "        return word_to_index[\"__EMPTY__\"] * (final_size - len(words)) + words\n",
    "    else:\n",
    "        return words + word_to_index[\"__EMPTY__\"] * (final_size - len(words))\n",
    "\n",
    "for epoch in xrange(20):\n",
    "    j = 0\n",
    "    all_costs = []\n",
    "    recent_costs = []\n",
    "    print \"Starting epoch %i...\" % epoch\n",
    "    for i, (left, phrase, right) in enumerate(x_train):\n",
    "        candidates = ttable[tuple(phrase)]\n",
    "        \"\"\"\n",
    "        new_cost = func_train([left[-3:]], [right[:3]], [[y_train[i]]], [good_probas], [[candidates[j][0]]], [bad_probas])\n",
    "        recent_costs.append(new_cost)\n",
    "        all_costs.append(new_cost)\n",
    "        \"\"\"\n",
    "        \"\"\"good_probas = candidates[[x for x, _ in candidates].index(y_train[i])][1]\n",
    "        \n",
    "        #new_cost = func_train([left[-3:]], [right[:3]], [[y_train[i]]], [good_probas], np.array([1.0], dtype=np.float32))\n",
    "        new_cost = func_train([good_probas], np.array([1.0], dtype=np.float32))\n",
    "        recent_costs.append(new_cost)\n",
    "        all_costs.append(new_cost)\n",
    "        \n",
    "        \n",
    "        for _ in xrange(1):\n",
    "            j = np.random.randint(len(candidates))\n",
    "            while candidates[j][0] == y_train[i]:\n",
    "                j = np.random.randint(len(candidates))\n",
    "            bad_probas = candidates[j][1]\n",
    "            #new_cost = func_train([left[-3:]], [right[:3]], [[candidates[j][0]]], [bad_probas], np.array([0.01], dtype=np.float32))\n",
    "            new_cost = func_train([bad_probas], np.array([0.01], dtype=np.float32))\n",
    "            recent_costs.append(new_cost)\n",
    "            all_costs.append(new_cost)\"\"\"\n",
    "        \n",
    "        context = left[-4:] + right[:4]\n",
    "        if len(context) == 0:\n",
    "            context = [\"__EMPTY__\"]\n",
    "        context = [word_to_index_en[x] for x in context]\n",
    "        \n",
    "        sizes = set([len(candidate[0]) for candidate in candidates])\n",
    "        sorted_candidates = {size:[c for c in candidates if len(c[0]) == size] for size in sizes}\n",
    "        \n",
    "        for _, batch in sorted_candidates.items():\n",
    "            #print [[word_to_index_cs[x] for x in b[0]] for b in batch]\n",
    "            context_matrix = [context for _ in xrange(len(batch))]\n",
    "            phrase_matrix = [context for _ in xrange(len(batch))]\n",
    "            vector_is_good = np.array([1.0 if y_train[i] == b[0] else 0.001 for b in batch], dtype=np.float32)\n",
    "            new_cost = func_train(context_matrix, phrase_matrix, [[word_to_index_cs[x] for x in b[0]] for b in batch], [b[1] for b in batch], vector_is_good)\n",
    "            recent_costs.append(new_cost)\n",
    "            all_costs.append(new_cost)\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print \"%i, average cost since last time: %f\" % (i, np.mean(recent_costs))\n",
    "            recent_costs = []\n",
    "    \n",
    "        if i % 50000 == 0:\n",
    "            positions_train = Parallel(n_jobs=num_cores)(delayed(compute_position_train)(i) for i in xrange(len(x_dev)))\n",
    "            positions_dev = Parallel(n_jobs=num_cores)(delayed(compute_position_dev)(i) for i in xrange(len(x_dev)))\n",
    "            score_train, score_dev = compute_final_score(positions_train), compute_final_score(positions_dev)\n",
    "            print score_train, score_dev\n",
    "    \n",
    "    print \"Epoch %i done. Average cost of this epoch: %f\" % (epoch, np.mean(all_costs))\n",
    "    positions_train = Parallel(n_jobs=num_cores)(delayed(compute_position_train)(i) for i in xrange(len(x_dev)))\n",
    "    positions_dev = Parallel(n_jobs=num_cores)(delayed(compute_position_dev)(i) for i in xrange(len(x_dev)))\n",
    "    score_train, score_dev = compute_final_score(positions_train), compute_final_score(positions_dev)\n",
    "    print score_train, score_dev\n",
    "    cPickle.dump(hidden_layer.weights.get_value(), open(model_path % (epoch, \"sh_second\", \"hidden_weights\", score_train, score_dev), \"w\"))\n",
    "    cPickle.dump(hidden_layer.bias.get_value(), open(model_path % (epoch, \"sh_second\", \"hidden_bias\", score_train, score_dev), \"w\"))\n",
    "    cPickle.dump(merge_layer.weights.get_value(), open(model_path % (epoch, \"sh_second\", \"merge_weights\", score_train, score_dev), \"w\"))\n",
    "    cPickle.dump(merge_layer.bias.get_value(), open(model_path % (epoch, \"sh_second\", \"merge_bias\", score_train, score_dev), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers_learning_rate = 0.01\n",
    "embedding_learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cores = 7 #multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_position(x, y):\n",
    "    left, phrase, right = x\n",
    "    good = y\n",
    "    candidates = ttable[tuple(phrase)]\n",
    "    left = [left for i in xrange(len(candidates))]\n",
    "    phrase = [phrase for i in xrange(len(candidates))]\n",
    "    right = [right for i in xrange(len(candidates))]\n",
    "    scores = np.array(func_feedforward(left, right, [[z[0]] for z in candidates], [z[1] for z in candidates]))\n",
    "    #scores = np.array(func_feedforward([z[1] for z in candidates]))\n",
    "    #scores = np.array(func_feedforward([z[1] for z in candidates]))\n",
    "    return (list(np.argsort(scores))[::-1].index([x for x, _ in candidates].index(good)) + 1)\n",
    "\n",
    "def compute_position(x, y):\n",
    "    left, phrase, right = x\n",
    "    good = y\n",
    "    candidates = ttable[tuple(phrase)]\n",
    "    \n",
    "    context = left[-4:] + right[:4]\n",
    "    if len(context) == 0:\n",
    "        context = [\"__EMPTY__\"]\n",
    "    context = [word_to_index_en[x] for x in context]\n",
    "    phrase = [word_to_index_en[x] for x in phrase]\n",
    "\n",
    "    sizes = set([len(candidate[0]) for candidate in candidates])\n",
    "    sorted_candidates = {size:[c for c in candidates if len(c[0]) == size] for size in sizes}\n",
    "    all_scores = []\n",
    "    \n",
    "    for _, batch in sorted_candidates.items():\n",
    "        context_matrix = [context for _ in xrange(len(batch))]\n",
    "        phrase_matrix = [phrase for _ in xrange(len(batch))]\n",
    "        #vector_is_good = np.array([1.0 if y_train[i] == b[0] else 0.001 for b in batch], dtype=np.float32)\n",
    "        scores = func_feedforward(context_matrix, phrase_matrix, [[word_to_index_cs[x] for x in b[0]] for b in batch], [b[1] for b in batch])\n",
    "        all_scores.extend([(tuple(tok), score) for tok, score in zip([b[0] for b in batch], scores)])\n",
    "    #print all_scores\n",
    "    #return\n",
    "    return [x[0] for x in sorted(all_scores, key=lambda x: x[1], reverse=True)].index(tuple(good)) + 1\n",
    "    #return (list(np.argsort(scores))[::-1].index([x for x, _ in candidates].index(tuple(good))) + 1)\n",
    "\n",
    "def compute_position_train(i):\n",
    "    return compute_position(x_train[i], y_train[i])\n",
    "\n",
    "def compute_position_dev(i):\n",
    "    return compute_position(x_dev[i], y_dev[i])\n",
    "\n",
    "def compute_best_words(x):\n",
    "    left, phrase, right = x\n",
    "    candidates = ttable[tuple(phrase)]\n",
    "    \n",
    "    context = left[-3:] + right[3:]\n",
    "    if len(context) == 0:\n",
    "        context = [\"__EMPTY__\"]\n",
    "    context = [word_to_index_en[x] for x in context]\n",
    "    phrase = [word_to_index_en[x] for x in phrase]\n",
    "\n",
    "    sizes = set([len(candidate[0]) for candidate in candidates])\n",
    "    sorted_candidates = {size:[c for c in candidates if len(c[0]) == size] for size in sizes}\n",
    "    all_scores = []\n",
    "    \n",
    "    for _, batch in sorted_candidates.items():\n",
    "        context_matrix = [context for _ in xrange(len(batch))]\n",
    "        phrase_matrix = [phrase for _ in xrange(len(batch))]\n",
    "        scores = func_feedforward(context_matrix, phrase_matrix, [[word_to_index_cs[x] for x in b[0]] for b in batch], [b[1] for b in batch])\n",
    "        all_scores.extend([(tuple(tok), score) for tok, score in zip([b[0] for b in batch], scores)])\n",
    "    #print all_scores\n",
    "    #return\n",
    "    \n",
    "    #return [\" \".join(x[0]) for x in sorted(all_scores, key=lambda x: x[1], reverse=True)]\n",
    "    return \" ||| \".join([\" \".join(x[0]) for x in sorted(all_scores, key=lambda x: x[1], reverse=True)])\n",
    "\n",
    "def compute_words_dev(i):\n",
    "    if i < 356:\n",
    "        return compute_best_words(x_dev[i])\n",
    "    elif i == 356:\n",
    "        return u\"orb치ne ||| orb치n ||| orb치na ||| orb치novi\"\n",
    "    elif i > 356 and i < 5421:\n",
    "        return compute_best_words(x_dev[i-1])\n",
    "    elif i == 5421:\n",
    "        return u\"spolupracovali ||| spolupracovala ||| spolupracovaly ||| spolupracoval\"\n",
    "    else:\n",
    "        return compute_best_words(x_dev[i-2])\n",
    "\n",
    "def compute_words_test(i):\n",
    "    return compute_best_words(x_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_dev_file_path = \"/home/guillaume/Documents/CMU/cours/MT/git/sp2015.11-731/hw4/output_dev_1.txt\"\n",
    "with open(output_dev_file_path, \"w\") as f:\n",
    "    f.write(u\"\\n\".join([compute_words_dev(i) for i in xrange(len(x_dev)+1)]).encode(\"utf8\"))\n",
    "    f.write(u\"\\n\")\n",
    "    f.write(u\"\\n\".join([compute_words_test(i) for i in xrange(len(x_test))]).encode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5791"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_position_dev(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aaa = merge_layer.weights.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1638297,\n",
       " -0.65674418,\n",
       " -2.0892873,\n",
       " -1.4943478,\n",
       " 0.90839487,\n",
       " 1.2587247,\n",
       " 1.552141,\n",
       " 1.1953601,\n",
       " 1.7400082,\n",
       " 0.69266123]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in merge_layer.weights.get_value()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = [1, 0, 0.3, 0]\n",
    "values = [2.07229996, 0.7576499, -0.2130859, 0.24015519, 0.18058282]\n",
    "values = [2.07229996, 0, 0, 0, 0]\n",
    "merge_layer.weights.set_value(np.array([[x] for x in values], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positions_train = Parallel(n_jobs=num_cores)(delayed(compute_position_train)(i) for i in xrange(len(x_dev)+1))\n",
    "positions_dev = Parallel(n_jobs=num_cores)(delayed(compute_position_dev)(i) for i in xrange(len(x_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_final_score(positions):\n",
    "    return np.mean([1.0 / x for x in positions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65728945775343628"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_final_score(positions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4075099062044697"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_final_score(positions_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57807477901939808"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_final_score(positions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41191242927781341"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_final_score(positions_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positions = []\n",
    "for x, y in zip(x_dev, y_dev):\n",
    "    candidates = [z[0] for z in sorted(ttable[tuple(x[1])], key=lambda y: np.sum(y[1]), reverse=True)]\n",
    "    positions.append(candidates.index(y) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37160985215792669"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs_wvectors = cPickle.load(open(\"/home/guillaume/workspace/NewDeep/word_vectors/polyglot-cs.pkl\", \"r\"))\n",
    "en_wvectors = cPickle.load(open(\"/home/guillaume/workspace/NewDeep/word_vectors/polyglot-en.pkl\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/home/guillaume/workspace/NewDeep/word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100004"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cs_wvectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24006841,  0.0170402 , -0.5328812 , ..., -0.22872141,\n",
       "         0.42429936, -1.06828964],\n",
       "       [ 0.04196003,  0.12553459, -0.10587557, ..., -0.22909568,\n",
       "         0.3757889 ,  0.24626437],\n",
       "       [ 0.1696005 ,  1.13571692,  0.93620867, ..., -0.68687445,\n",
       "         0.35930997, -0.01402457],\n",
       "       ..., \n",
       "       [-0.02400243,  0.02764406,  0.11886688, ...,  0.05557791,\n",
       "        -0.20100911,  0.09282091],\n",
       "       [-0.22458398,  0.23377183,  0.15628465, ...,  0.26328474,\n",
       "        -0.17130157, -0.32867849],\n",
       "       [-0.03621942, -0.28310931, -0.01432774, ..., -0.10810753,\n",
       "        -0.37851715,  0.22238818]], dtype=float32)"
      ]
     },
     "execution_count": 973,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_wvectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202089, 50)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_closests(word, english=True):\n",
    "    vector = en_vectors[mapping_to_ext_en[word]]\n",
    "    best_indexes_en = np.argsort(en_vectors.dot(vector))[::-1][:10]\n",
    "    best_indexes_cs = np.argsort(cs_vectors.dot(vector))[::-1][:10]\n",
    "    for i in best_indexes_en:\n",
    "        print en_words[i]\n",
    "    for i in best_indexes_cs:\n",
    "        print cs_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dead\n",
      "dying\n",
      "nameless\n",
      "kilter\n",
      "ixtapaluca\n",
      "destroyed\n",
      "thrown\n",
      "ignites\n",
      "humanoids\n",
      "died\n",
      "h치c칤ch\n",
      "poseta\n",
      "nezem콏ela\n",
      "vstal\n",
      "zachr치n칤me\n",
      "prolet칤\n",
      "st콏elen\n",
      "mrtv치\n",
      "drez칤nu\n",
      "pust칠m\n"
     ]
    }
   ],
   "source": [
    "find_closests(\"dead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'gauge', u'situation', u'proportion'],\n",
       " [u'believe'],\n",
       " [u'dissatisfaction',\n",
       "  u'prime',\n",
       "  u'minister',\n",
       "  u'managed',\n",
       "  u'without',\n",
       "  u'having',\n",
       "  u'put',\n",
       "  u'government',\n",
       "  u'office']]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[x for x in l if x in word_to_index_en and (train_english_tok_count[x] < threshold or i == 1)] for i, l in enumerate(dev_input_lines[357])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
